{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk.corpus\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv(\"train_F3WbcTw.csv\")\n",
    "file = file.drop([\"unique_hash\",\"drug\"],axis=1)\n",
    "file2 = pd.read_csv(\"test_tOlRoBf.csv\")\n",
    "file2 = file2.drop([\"unique_hash\",\"drug\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file[\"clean text\"] = file[\"text\"].apply( lambda x : x.lower())\n",
    "file[\"clean text\"] = file[\"clean text\"].str.replace(\"[^a-zA-Z#]\",\" \")\n",
    "file[\"clean text\"] = file[\"clean text\"].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\n",
    "file[\"clean text\"] = file[\"clean text\"].apply( lambda x : x.split(\" \"))\n",
    "stemmer = PorterStemmer()\n",
    "file[\"clean text\"] = file[\"clean text\"].apply(lambda x:[stemmer.stem(i) for i in x] )\n",
    "file[\"clean text\"] = file[\"clean text\"].apply(lambda x:' '.join(x) )\n",
    "file = file.drop([\"text\"], axis = 1)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=None, stop_words='english', ngram_range=(1, 2))\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(file[\"clean text\"])\n",
    "\n",
    "file2[\"clean text\"] = file2[\"text\"].apply( lambda x : x.lower())\n",
    "file2[\"clean text\"] = file2[\"clean text\"].str.replace(\"[^a-zA-Z#]\",\" \")\n",
    "file2[\"clean text\"] = file2[\"clean text\"].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\n",
    "file2[\"clean text\"] = file2[\"clean text\"].apply( lambda x : x.split(\" \"))\n",
    "\n",
    "file2[\"clean text\"] = file2[\"clean text\"].apply(lambda x:[stemmer.stem(i) for i in x] )\n",
    "file2[\"clean text\"] = file2[\"clean text\"].apply(lambda x:' '.join(x) )\n",
    "file2 = file2.drop([\"text\"], axis = 1)\n",
    "\n",
    "\n",
    "# bag-of-words feature matrix\n",
    "bow2 = bow_vectorizer.transform(file2[\"clean text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score: 0.4506488853503184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "y = file[\"sentiment\"]\n",
    "\n",
    "#splitting data into training and validation set\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(bow,y,random_state=42, test_size=0.3)\n",
    "lreg = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "\n",
    "lreg.fit(xtrain, ytrain) # training the model\n",
    "predict = lreg.predict(xtest)\n",
    "from sklearn import metrics\n",
    "#metrics.accuracy_score(predict,ytest)\n",
    "print(\"macro f1 score:\",metrics.f1_score(predict,ytest,average = \"macro\"))\n",
    "#print(\"micro f1 score:\",metrics.f1_score(predict,ytest,average = \"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 0 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2924"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predict)\n",
    "len(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 0, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'unique_hash':file3['unique_hash'],'sentiment':predict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2924"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"predict.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
